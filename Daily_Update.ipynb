{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED REPORT OF THE CRYPTO-CURRENCIES PERIODIC UPDATE:\n",
      "allCoins table extracted from coinmarketcap.com in 2.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Website and Source Code URL: 389 Rows processed - 25% Complete and time taken is  2.31 minutes\n",
      "Adding Website and Source Code URL: 779 Rows processed - 50% Complete and time taken is  6.54 minutes\n",
      "Adding Website and Source Code URL: 1169 Rows processed - 75% Complete and time taken is  8.74 minutes\n",
      "Adding Website and Source Code URL: 1403 Rows processed - 90% Complete and time taken is  10.11 minutes\n",
      "Website and Source Code URL for all Crypto-currencies retrieved in 11.1 minutes\n",
      "[u'allCoins', u'coinsHist_2013', u'coinsHist_2014', u'coinsHist_2015', u'coinsHist_2016', u'coinsHist_2017', u'coinsHist_2018']\n",
      "-------------------------------------------------------------\n",
      "NO NEW COINS ADDED TODAY TO coinmarketcap.com\n",
      "Web Scraping Engine for historical data: 400 Rows processed - 25% Complete and time taken is  3.94 minutes\n",
      "Web Scraping Engine for historical data: 801 Rows processed - 50% Complete and time taken is  7.75 minutes\n",
      "Web Scraping Engine for historical data: 1201 Rows processed - 75% Complete and time taken is  11.5 minutes\n",
      "Web Scraping Engine for historical data: 1441 Rows processed - 90% Complete and time taken is  13.76 minutes\n",
      "HTTPError - Data missing for the coin: farstcoin\n",
      "Extracted all Crypto-currencies data from the web for today in 14.86 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/pandas/io/sql.py:531: FutureWarning: the 'flavor' parameter is deprecated and will be removed in a future version, as 'sqlite' is the only supported option when SQLAlchemy is not installed.\n",
      "  _validate_flavor_parameter(flavor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED THE PERIODIC UPDATE IN 26.0 MINUTES\n",
      "We have historical information for 1602 Crypto-currencies in our database\n",
      "Details for 43 crypto-currencies are removed from coinmarketcap.com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# # Periodic Dump\n",
    "\n",
    "# ## Setting up the environment\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import httplib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import ObjectNotExecutableError\n",
    "from sqlalchemy import MetaData, table\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import smtplib\n",
    "from email.MIMEMultipart import MIMEMultipart\n",
    "from email.MIMEText import MIMEText\n",
    "periodic_start = time.time()\n",
    "\n",
    "f= open(\"Periodic_Update.txt\",\"w+\")\n",
    "def append_to_file(m):\n",
    "    m = str(m)\n",
    "    m = re.sub(\"[(,)']\", \"\", m)\n",
    "    print m\n",
    "    f.write(m)\n",
    "    f.write(\"\\n \\n\")\n",
    "\n",
    "m = \"DETAILED REPORT OF THE CRYPTO-CURRENCIES PERIODIC UPDATE:\"\n",
    "m3 = \"-------------------------------------------------------------\"\n",
    "\n",
    "append_to_file(m)\n",
    "\n",
    "\n",
    "# ## 1. Web Extract: Today's All Coins Table\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def CoinNames(x):\n",
    "    \"\"\"Gets ID's of all coins on cmc\"\"\"\n",
    "\n",
    "    data = []\n",
    "    response = requests.get(\"https://api.coinmarketcap.com/v1/ticker/?limit=0\")\n",
    "    respJSON = json.loads(response.text)\n",
    "    for i in respJSON:\n",
    "        if x == 'Coin':\n",
    "            data.append(i['id'])\n",
    "        if x == 'Symbol':\n",
    "            data.append(i['symbol'])\n",
    "        if x == 'Circulating_Supply':\n",
    "            data.append(i['available_supply'])\n",
    "        if x == 'Price':\n",
    "            data.append(i['price_usd'])\n",
    "        if x == 'Market_Cap':\n",
    "            data.append(i['market_cap_usd'])\n",
    "        if x == 'Rate_1h':\n",
    "            data.append(i['percent_change_1h'])\n",
    "        if x == 'Rate_24h':\n",
    "            data.append(i['percent_change_24h'])\n",
    "        if x == 'Rate_7d':\n",
    "            data.append(i['percent_change_7d'])\n",
    "        if x == 'coins':\n",
    "            data.append(i['id'])\n",
    "        if x == 'Id':\n",
    "            data.append(i['rank'])\n",
    "        if x == 'symbols':\n",
    "            data.append(i['symbol'])\n",
    "            \n",
    "    if x == 'coins':\n",
    "        return data\n",
    "    if x == 'symbols':\n",
    "        return data\n",
    "    \n",
    "    data = pd.DataFrame(np.array(data), columns = [x])\n",
    "    return data\n",
    "\n",
    "Id = CoinNames('Id')\n",
    "Names = CoinNames('Coin')\n",
    "Symbols = CoinNames('Symbol')\n",
    "market_cap_usd = CoinNames('Market_Cap')\n",
    "price_usd = CoinNames('Price')\n",
    "cir_supply = CoinNames('Circulating_Supply')\n",
    "percent_change_1h = CoinNames('Rate_1h')\n",
    "percent_change_24h = CoinNames('Rate_24h')\n",
    "percent_change_7d = CoinNames('Rate_7d')\n",
    "allCoins = pd.concat([Id, Names, Symbols, market_cap_usd, price_usd, cir_supply, percent_change_1h, percent_change_24h, percent_change_7d], axis=1)\n",
    "cols = allCoins.columns.drop('Coin', 'Symbol')\n",
    "allCoins[cols] = allCoins[cols].apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "allCoins['Id'] = range(1, len(allCoins)+1)\n",
    "allCoins['Id'] = allCoins['Id'].astype(int)\n",
    "\n",
    "start = (time.time() - start)    \n",
    "m = \"allCoins table extracted from coinmarketcap.com in\", round(start, 2), \"seconds\"\n",
    "append_to_file(m)\n",
    "\n",
    "\n",
    "# ### 1.1. Website and Source Code URL\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "Home_URL = pd.DataFrame(columns=['Website'])\n",
    "Source_Code_link = pd.DataFrame(columns=['Source Code'])\n",
    "\n",
    "start = time.time()\n",
    "coins = CoinNames('coins')\n",
    "symbols = CoinNames('symbols')\n",
    "#n = 500\n",
    "for i in range(0,len(coins)): #len(allCoins)\n",
    "        try:\n",
    "            html_page = urllib2.urlopen(\"https://coinmarketcap.com/currencies/\"+coins[i]+\"/\")\n",
    "            soup = BeautifulSoup(html_page)\n",
    "            link1 = soup.find('a', text='Website')\n",
    "            link2 = soup.find('a', text='Source Code')\n",
    "            \n",
    "            if link1 is None:\n",
    "                Home_URL.loc[i] = \"Not Available\"\n",
    "            else:\n",
    "                Home_URL.loc[i] = link1.get('href')\n",
    "                \n",
    "            if link2 is None:\n",
    "                Source_Code_link.loc[i] = \"Not Available\"\n",
    "            else:\n",
    "                Source_Code_link.loc[i] = link2.get('href')\n",
    "            \n",
    "            if(i == round(len(allCoins)/4, 0)):\n",
    "                print \"Adding Website and Source Code URL:\", i, \"Rows processed - 25% Complete and time taken is \", round((time.time() - start) / 60,2),\"minutes\"\n",
    "            if(i == round(len(allCoins)/2, 0)):\n",
    "                print \"Adding Website and Source Code URL:\", i, \"Rows processed - 50% Complete and time taken is \", round((time.time() - start) / 60,2),\"minutes\"\n",
    "            if(i == round((len(allCoins)*75)/100, 0)):\n",
    "                print \"Adding Website and Source Code URL:\", i, \"Rows processed - 75% Complete and time taken is \", round((time.time() - start) / 60,2),\"minutes\"\n",
    "            if(i == round((len(allCoins)*9)/10, 0)):\n",
    "                print \"Adding Website and Source Code URL:\", i, \"Rows processed - 90% Complete and time taken is \", round((time.time() - start) / 60,2),\"minutes\"\n",
    "    #    print link.get('href')\n",
    "        except httplib.IncompleteRead as error:\n",
    "            m = 'There was an IncompleteRead Exception while extracting Crypto-Currency number', i\n",
    "            append_to_file(m)\n",
    "            html_page = error.partial\n",
    "            \n",
    "start = (time.time() - start) / 60    \n",
    "m = \"Website and Source Code URL for all Crypto-currencies retrieved in\", round(start, 2), \"minutes\"\n",
    "append_to_file(m)\n",
    "\n",
    "\n",
    "# ### 1.3. Merge All Data\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "Home_URL1 = Home_URL.reset_index(drop=True)\n",
    "Home_URL1[\"Id\"] = range(0,len(allCoins))\n",
    "Home_URL1.set_index('Id', inplace=True)\n",
    "\n",
    "Source_Code_link1 = Source_Code_link.reset_index(drop=True)\n",
    "Source_Code_link1[\"Id\"] = range(0,len(allCoins))\n",
    "Source_Code_link1.set_index('Id', inplace=True)\n",
    "\n",
    "\n",
    "today_allCoins = pd.concat([allCoins, Home_URL1, Source_Code_link1], axis=1)\n",
    "\n",
    "\n",
    "# ### 1.5. Connect, Compare and Append\n",
    "\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "engine = create_engine('mysql://root:root@localhost:3306/crypto', pool_recycle=280) \n",
    "con = engine.connect()\n",
    "print(engine.table_names())\n",
    "metadata = MetaData()\n",
    "\n",
    "allCoins_yest = pd.read_sql('SELECT * FROM allCoins', con=con)\n",
    "allCoins = today_allCoins\n",
    "common = allCoins.merge(allCoins_yest,on=['Coin','Coin'])\n",
    "temp = allCoins[(~allCoins.Coin.isin(common.Coin))&(~allCoins.Coin.isin(common.Coin))]\n",
    "temp['Id'] = range(len(allCoins_yest)+1,len(allCoins_yest)+len(temp['Id'])+1)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "def allCoins_toSQL_Append():\n",
    "    try:\n",
    "        tosql = engine.execute(temp.to_sql('allCoins', con, flavor='sqlite', if_exists='append', index=False))\n",
    "    except ObjectNotExecutableError as error:\n",
    "        pass\n",
    "    m = \"Number of coins newly added to coinmarketcap.com:\", len(temp)    \n",
    "    m2 = temp.iloc[:, 0:5]\n",
    "    m4 = \"SUCCESS MESSAGE: NEW COINS APPENDED TO THE MYSQL DATABASE\"\n",
    "    append_to_file(m)\n",
    "    append_to_file(m3)\n",
    "    append_to_file(m2)\n",
    "    append_to_file(m3)\n",
    "    append_to_file(m4)\n",
    "\n",
    "if len(temp) > 0:       \n",
    "    allCoins_toSQL_Append()\n",
    "else:\n",
    "    m = \"NO NEW COINS ADDED TODAY TO coinmarketcap.com\"\n",
    "    append_to_file(m3)\n",
    "    append_to_file(m)\n",
    "\n",
    "\n",
    "# ## 2. Historical Table\n",
    "\n",
    "# ### 2.1. Extract today's data from the web\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "end_date = datetime.datetime.today().strftime('%Y%m%d')\n",
    "start_date = datetime.datetime.strptime(end_date, '%Y%m%d').date() - datetime.timedelta(1)\n",
    "start_date = str(start_date)\n",
    "start_date = re.sub(\"-\", \"\", start_date)\n",
    "\n",
    "allCoins = pd.read_sql('SELECT * FROM allCoins', con=con)\n",
    "\n",
    "def coinsHistory_update(start_date, end_date):\n",
    "    coinsHist = pd.DataFrame() #Size not known\n",
    "    start = time.time()\n",
    "    for i in range(0,len(coins)): #len(allCoins)\n",
    "        try:\n",
    "            quote_page1 = 'https://coinmarketcap.com/currencies/'+coins[i]+'/historical-data/?start='+start_date+'&end='+end_date\n",
    "            page1 = urllib2.urlopen(quote_page1)\n",
    "            soup1 = BeautifulSoup(page1, 'html.parser')\n",
    "            table1 = soup1.find('table', attrs={\"class\":\"table\"})\n",
    "            new_table2 = pd.DataFrame(columns=range(0,7), index = range(0,1+1)) #no_of_days+1\n",
    "            row_marker1 = 0\n",
    "            for row1 in table1.find_all('tr'):\n",
    "                column_marker1 = 0\n",
    "                columns1 = row1.find_all('td')\n",
    "                for column in columns1:\n",
    "                    new_table2.iat[row_marker1,column_marker1] = column.get_text() #iat for indexing - faster version of iloc\n",
    "                    column_marker1 += 1\n",
    "                row_marker1 += 1\n",
    "            if(i == round(len(allCoins)/4, 0)):\n",
    "                print \"Web Scraping Engine for historical data:\", i, \"Rows processed - 25% Complete and time taken is \", round((time.time() - start) / 60,2),\"minutes\"\n",
    "            if(i == round(len(allCoins)/2, 0)):\n",
    "                print \"Web Scraping Engine for historical data:\", i, \"Rows processed - 50% Complete and time taken is \", round((time.time() - start) / 60,2),\"minutes\"\n",
    "            if(i == round((len(allCoins)*75)/100, 0)):\n",
    "                print \"Web Scraping Engine for historical data:\", i, \"Rows processed - 75% Complete and time taken is \", round((time.time() - start) / 60,2),\"minutes\"\n",
    "            if(i == round((len(allCoins)*9)/10, 0)):\n",
    "                print \"Web Scraping Engine for historical data:\", i, \"Rows processed - 90% Complete and time taken is \", round((time.time() - start) / 60,2),\"minutes\"\n",
    "                \n",
    "            new_table2 = new_table2.drop(new_table2.index[[0]])\n",
    "            new_table2['Coin'] = coins[i]\n",
    "            new_table2['Symbol'] = symbols[i]\n",
    "            coinsHist = coinsHist.append(new_table2)\n",
    "        #    print 'Appended coin number:', i+1\n",
    "\n",
    "            \n",
    "        except httplib.IncompleteRead as error:\n",
    "            m = \"There was an IncompleteRead Exception while extracting Crypto-Currency number\", i\n",
    "            append_to_file(m)\n",
    "            page1 = error.partial\n",
    "        except urllib2.HTTPError as e:\n",
    "            m = e, \"- Data missing for the coin:\", coins[i].encode(\"utf-8\")\n",
    "            append_to_file(m)\n",
    "            continue\n",
    "  \n",
    "        \n",
    "    start = (time.time() - start) / 60    \n",
    "    m = \"Extracted all Crypto-currencies data from the web, for today in\", round(start, 2), \"minutes\"\n",
    "    append_to_file(m)\n",
    "    header = []\n",
    "    columns = table1.find_all('th')\n",
    "    for column in columns:\n",
    "        header.append(column.get_text())\n",
    "    coinsHist.columns.values[0:7] = header\n",
    "    \n",
    "    coinsHist = coinsHist.dropna(axis=0, thresh=7)\n",
    "    coinsHist = coinsHist.merge(allCoins[['Id','Coin']],left_on='Coin',right_on='Coin',how='left').fillna('')\n",
    "    \n",
    "\n",
    "    cols1 = coinsHist.columns.drop('Coin', 'Symbol')\n",
    "    coinsHist[cols1] = coinsHist[cols1].apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    return coinsHist\n",
    "\n",
    "coinsHist_new = coinsHistory_update(start_date, end_date)\n",
    "\n",
    "\n",
    "# ### 2.2. Append today's Information to MySQL\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def coinsHist_update():\n",
    "    try:\n",
    "        tosql_2018 = engine.execute(coinsHist_new.to_sql('coinsHist_2018', con, flavor='sqlite', if_exists='append', index=False))\n",
    "    except ObjectNotExecutableError as error:\n",
    "        pass\n",
    "\n",
    "\n",
    "coinsHist_update()\n",
    "periodic_start = (time.time() - periodic_start) / 60    \n",
    "m = \"COMPLETED THE PERIODIC UPDATE IN\", round(periodic_start, 2),\"MINUTES\"\n",
    "append_to_file(m)\n",
    "fromaddr = \"mughundhan.testing@gmail.com\"\n",
    "toaddr = \"mughundhanc@gmail.com\"\n",
    "msg = MIMEMultipart()\n",
    "msg['From'] = fromaddr\n",
    "msg['To'] = toaddr\n",
    "msg['Subject'] = \"Crypto-currencies: Automated Periodic Update\"\n",
    "body = \"All details pertaining to the Crypto-currencies periodic update are appended to the txt file and attached along with this e-mail. Kindly lookout for the attachment.\"\n",
    "msg.attach(MIMEText(body, 'plain'))\n",
    "filename = \"Periodic_Update.txt\"\n",
    "f = file(filename)\n",
    "attachment = MIMEText(f.read())\n",
    "attachment.add_header('Content-Disposition', 'attachment', filename=filename)           \n",
    "msg.attach(attachment)\n",
    "server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "server.ehlo()\n",
    "server.starttls()\n",
    "server.ehlo()\n",
    "server.login(\"mughundhan.testing@gmail.com\", \"123mictesting\")\n",
    "text = msg.as_string()\n",
    "server.sendmail(fromaddr, toaddr, text)\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "print 'We have historical information for', len(allCoins),'Crypto-currencies in our database'\n",
    "print 'Details for', len(allCoins) - len(coins),'crypto-currencies are removed from coinmarketcap.com'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
